{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "import json \n",
    "import time\n",
    "import requests \n",
    "import requests.auth\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortniteCompetitive_df = pd.read_csv('data/fortniteCompetitive.csv')\n",
    "fortniteBR_df = pd.read_csv('data/fortniteBR.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Post Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SUBREDDIT = 'FortniteCompetitive'\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "columns = ['author', 'created_utc', 'id', 'num_comments', 'permalink', 'score', 'title', 'selftext', 'subreddit']\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Uncomment if starting from the beginning\n",
    "# checkpoint = {'date' : 1561584036, \n",
    "#               'count': count}\n",
    "\n",
    "# with open('checkpoint.txt', 'w') as outfile:\n",
    "#     json.dump(checkpoint, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('checkpoint.txt') as file: \n",
    "    checkpoint = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params={'size':'500', \n",
    "        'subreddit': SUBREDDIT, \n",
    "        'num_comments':'>10', \n",
    "        'before' : checkpoint['date']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scrape(params):\n",
    "    response = [1]\n",
    "    count = checkpoint['count']\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, params=params)\n",
    "        print('Status code: ' + str(response.status_code))\n",
    "        response = response.json()\n",
    "        length = len(response['data'])\n",
    "        print('Data length: ' + str(length))\n",
    "        \n",
    "        if length == 0: \n",
    "            print('Scraping finished.')\n",
    "            break\n",
    "        \n",
    "        df = pd.DataFrame(response['data'])\n",
    "        df = df[columns]\n",
    "        \n",
    "        filename = SUBREDDIT + str(count)\n",
    "        path = 'data/' + filename \n",
    "        df.to_csv(path)\n",
    "        print('File named: ' + filename + ' saved')\n",
    "        \n",
    "        count = count + 1\n",
    "        checkpoint['count'] = count\n",
    "        \n",
    "        earliest = length  - 1\n",
    "        checkpoint['date'] = response['data'][earliest]['created_utc']\n",
    "        params['before'] = checkpoint['date']\n",
    "        \n",
    "        with open('checkpoint.txt', 'w') as outfile:\n",
    "            json.dump(checkpoint, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Data length: 0\n"
     ]
    }
   ],
   "source": [
    "scrape(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Merge all separate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pull_csvs(subreddit, count): \n",
    "    path = 'data/'    \n",
    "    filename = subreddit + str(0)\n",
    "    \n",
    "    if subreddit == 'FortNiteBR': \n",
    "        filename = '\\data\\\\' + filename \n",
    "        \n",
    "    master = pd.read_csv(path + filename)\n",
    "    \n",
    "    for csv in range(1, count): \n",
    "        filename = subreddit + str(csv)\n",
    "        if (subreddit == 'FortNiteBR') & (csv <= 214): \n",
    "            filename = '\\data\\\\' + filename \n",
    "        \n",
    "        df = pd.read_csv(path + filename)\n",
    "        master = pd.concat([master, df])\n",
    "\n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_csvs(subreddit, count, start=0): \n",
    "    path = 'data/' \n",
    "    \n",
    "    for csv in range(start, count): \n",
    "        filename = subreddit + str(csv)\n",
    "        \n",
    "        if (subreddit == 'FortNiteBR') & (csv <= 214): \n",
    "            filename = '\\data\\\\' + filename \n",
    "        \n",
    "        os.remove(path + filename)\n",
    "        \n",
    "    return 'Removed ' + str(count) + ' files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fortniteCompetitive_df = pull_csvs('FortniteCompetitive', 63)\n",
    "fortniteBR_df = pull_csvs('FortNiteBR', 430)\n",
    "\n",
    "fortniteCompetitive_df.to_csv('data/fortniteCompetitive.csv')\n",
    "fortniteBR_df.to_csv('data/fortniteBR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 63 files'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_csvs('FortniteCompetitive', 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 430 files'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_csvs('FortNiteBR', 430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_comments = 'https://oauth.reddit.com/r/fortniteCompetitive/comments/article'\n",
    "BATCH_SIZE = 20\n",
    "BATCH_NO = 0\n",
    "COLUMNS = ['author', 'body', 'created_utc', 'id', 'parent_id', 'score', 'subreddit', 'permalink']\n",
    "\n",
    "base_headers = {\"Authorization\": \"bearer 72466118-BGb8Kd7V0GF973kFmdLR0aepCdY\", \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
    "headers = set_auth(headers)\n",
    "params = {\n",
    "    'article':'',\n",
    "    'context':8,\n",
    "    'showedits':'True',\n",
    "    'showmore':'False',\n",
    "    'limit' : 1000,\n",
    "    'sort':'confidence',\n",
    "    'threaded':'True',\n",
    "    'truncate':50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint(): \n",
    "    #resume scraping from prev. location or start new \n",
    "    if (os.path.exists('comment_checkpoint.txt')): \n",
    "        with open('comment_checkpoint.txt') as file: \n",
    "            checkpoint = json.load(file)\n",
    "    else: \n",
    "        checkpoint = {'batch_no' : 0}\n",
    "        with open('comment_checkpoint.txt', 'w') as outfile: \n",
    "            json.dump(checkpoint, outfile)\n",
    "            \n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auth():\n",
    "    client_auth = requests.auth.HTTPBasicAuth('NELdoctFZ_tqVw', 'JgSF4SwJdrCsnuOSggxFoMIbzsA')\n",
    "    post_data = {\"grant_type\": \"password\", \"username\": \"jeromeco\", \"password\": \"Skicat12\"}\n",
    "    headers = {\"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
    "    response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "    r = response.json()\n",
    "    return r['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_auth(headers):\n",
    "    headers['Authorization'] = 'bearer ' + get_auth()\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is purposefully blocking\n",
    "def check_limit(id): \n",
    "    params['article'] = id\n",
    "    response = requests.get(url_comments, headers=headers, params=params)\n",
    "    limit = response.headers['x-ratelimit-remaining']\n",
    "    time_remaining = response.headers['x-ratelimit-reset']\n",
    "    return float(limit), float(time_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2df(response): \n",
    "    comment_queue = response[:] \n",
    "    post = comment_queue.pop(0) # Seed with top-level\n",
    "    comments = []\n",
    "\n",
    "    while comment_queue:\n",
    "        #get comment of queue\n",
    "        try : \n",
    "            comment = comment_queue.pop(0)\n",
    "            comment = comment['data']\n",
    "        except: \n",
    "            comment = comment_queue.pop(0)\n",
    "\n",
    "        #append new comment as a dict to list \n",
    "        if 'body' in comment:\n",
    "            new_comment = {k: comment[k] for k in COLUMNS}    \n",
    "            comments.append(new_comment)\n",
    "\n",
    "        #get children / replies of current comment\n",
    "        if 'children' in comment: \n",
    "            comment = comment['children']\n",
    "            comment_queue.extend(comment)\n",
    "        elif 'replies' in comment: \n",
    "            if len(comment['replies']) > 0: \n",
    "                comment = comment['replies']['data']['children']\n",
    "                comment_queue.extend(comment)\n",
    "        else: \n",
    "            print('error')\n",
    "    \n",
    "    return pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_comment(id, session): \n",
    "    params['article'] = id\n",
    "    async with session.get(url_comments, headers=headers, params=params) as resp:\n",
    "        resp.raise_for_status()\n",
    "        response = await resp.json()  \n",
    "        return json2df(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_batch(batch):  \n",
    "    limit, time_remaining = check_limit('8yrb5e')\n",
    "    if limit < BATCH_SIZE: \n",
    "        print('API LIMIT REACHED. \\nSleeping for: ' + str(time_remaining))\n",
    "        time.sleep(time_remaining)\n",
    "    async with ClientSession() as session:\n",
    "        dfs = []\n",
    "        \n",
    "        for id in batch: \n",
    "            dfs.append(scrape_comment(id, session))\n",
    "        \n",
    "        master = await asyncio.gather(*dfs)\n",
    "        df = pd.concat(master)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_all_comments(posts): \n",
    "    ids = posts['id']\n",
    "    subreddit = posts['subreddit'][0]\n",
    "    checkpoint = get_checkpoint()\n",
    "    \n",
    "    #setup filename structure for saved csvs\n",
    "    path = 'data/' + subreddit +'Comments'\n",
    "    \n",
    "    #iterate through posts to get all comments\n",
    "    x, y = checkpoint['batch_no'], math.ceil(len(ids) / BATCH_SIZE)\n",
    "    for i in tqdm(range(x, y)): \n",
    "        lo = i * BATCH_SIZE\n",
    "        hi = min(len(ids), BATCH_SIZE * (i + 1)) \n",
    "        batch = ids[lo:hi]\n",
    "        \n",
    "        df = await handle_batch(batch)\n",
    "        df.to_csv(path + str(i))\n",
    "        \n",
    "        checkpoint['batch_no'] = i + 1\n",
    "        with open('comment_checkpoint.txt', 'w') as outfile: \n",
    "            json.dump(checkpoint, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc44d4199b2a44e39e361cd514d25beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API LIMIT REACHED. \n",
      " Sleeping for: 230.0\n",
      "API LIMIT REACHED. \n",
      " Sleeping for: 542.0\n"
     ]
    }
   ],
   "source": [
    "await get_all_comments(fortniteCompetitive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object get_all_comments at 0x1114eb448>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_all_comments(fortniteBR_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 1575 files'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fortniteCompetitiveComments_df = pull_csvs('FortniteCompetitiveComments', 1575)\n",
    "fortniteCompetitiveComments_df.to_csv('data/fortniteCompComments')\n",
    "remove_csvs('FortniteCompetitiveComments', 1575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "843019"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fortniteCompetitiveComments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       t3_c5vyi4\n",
       "1       t3_c5vyi4\n",
       "2       t3_c5vyi4\n",
       "3       t3_c5vyi4\n",
       "4       t3_c5vyi4\n",
       "5       t3_c5vyi4\n",
       "6       t3_c5vyi4\n",
       "7       t3_c5vyi4\n",
       "8       t3_c5vyi4\n",
       "9       t3_c5vyi4\n",
       "10     t1_es4jmgr\n",
       "11     t1_es4k7vi\n",
       "12     t1_es4k7vi\n",
       "13     t1_es4fe68\n",
       "14     t1_es4rq13\n",
       "15     t1_es4rq13\n",
       "16     t1_es5xcqh\n",
       "17     t1_es4ffxu\n",
       "18     t1_es4fmjy\n",
       "19     t1_es4jsgi\n",
       "20     t1_es4kq41\n",
       "21      t3_c5vaq9\n",
       "22      t3_c5vaq9\n",
       "23      t3_c5vaq9\n",
       "24      t3_c5vaq9\n",
       "25      t3_c5vaq9\n",
       "26      t3_c5vaq9\n",
       "27     t1_es4bamp\n",
       "28     t1_es4aslr\n",
       "29     t1_es4dwjw\n",
       "          ...    \n",
       "530    t1_e08do8m\n",
       "531    t1_e08do8m\n",
       "532    t1_e08g932\n",
       "533    t1_e08f4ke\n",
       "534    t1_e099vhr\n",
       "535    t1_e09bpke\n",
       "536    t1_e08j9tt\n",
       "537    t1_e08ipk8\n",
       "538    t1_e09djgi\n",
       "539    t1_e08fvkf\n",
       "540    t1_e08fvkf\n",
       "541    t1_e09axlj\n",
       "542    t1_e08tlun\n",
       "543    t1_e08gp1r\n",
       "544    t1_e09au68\n",
       "545    t1_e08j68p\n",
       "546    t1_e08j68p\n",
       "547     t3_8p3q6f\n",
       "548     t3_8p3q6f\n",
       "549     t3_8p3q6f\n",
       "550     t3_8p3q6f\n",
       "551     t3_8p3q6f\n",
       "552     t3_8p3q6f\n",
       "553     t3_8p3q6f\n",
       "554     t3_8p3q6f\n",
       "555    t1_e087kse\n",
       "556    t1_e088hpc\n",
       "557    t1_e088lh6\n",
       "558    t1_e088n96\n",
       "559    t1_e088ygl\n",
       "Name: parent_id, Length: 843019, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fortniteCompetitiveComments_df['parent_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
