{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "import json \n",
    "import time\n",
    "import requests \n",
    "import requests.auth\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Post Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SUBREDDIT = 'FortniteCompetitive'\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "columns = ['author', 'created_utc', 'id', 'num_comments', 'permalink', 'score', 'title', 'selftext', 'subreddit']\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Uncomment if starting from the beginning\n",
    "# checkpoint = {'date' : 1561584036, \n",
    "#               'count': count}\n",
    "\n",
    "# with open('checkpoint.txt', 'w') as outfile:\n",
    "#     json.dump(checkpoint, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('checkpoint.txt') as file: \n",
    "    checkpoint = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params={'size':'500', \n",
    "        'subreddit': SUBREDDIT, \n",
    "        'num_comments':'>10', \n",
    "        'before' : checkpoint['date']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scrape(params):\n",
    "    response = [1]\n",
    "    count = checkpoint['count']\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, params=params)\n",
    "        print('Status code: ' + str(response.status_code))\n",
    "        response = response.json()\n",
    "        length = len(response['data'])\n",
    "        print('Data length: ' + str(length))\n",
    "        \n",
    "        if length == 0: \n",
    "            print('Scraping finished.')\n",
    "            break\n",
    "        \n",
    "        df = pd.DataFrame(response['data'])\n",
    "        df = df[columns]\n",
    "        \n",
    "        filename = SUBREDDIT + str(count)\n",
    "        path = 'data/' + filename \n",
    "        df.to_csv(path)\n",
    "        print('File named: ' + filename + ' saved')\n",
    "        \n",
    "        count = count + 1\n",
    "        checkpoint['count'] = count\n",
    "        \n",
    "        earliest = length  - 1\n",
    "        checkpoint['date'] = response['data'][earliest]['created_utc']\n",
    "        params['before'] = checkpoint['date']\n",
    "        \n",
    "        with open('checkpoint.txt', 'w') as outfile:\n",
    "            json.dump(checkpoint, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Data length: 0\n"
     ]
    }
   ],
   "source": [
    "scrape(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Merge all separate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pull_csvs(subreddit, count, start=0): \n",
    "    path = 'data/'    \n",
    "    filename = subreddit + str(0)\n",
    "    \n",
    "    if subreddit == 'FortNiteBR': \n",
    "        filename = '\\data\\\\' + filename \n",
    "        \n",
    "    master = pd.read_csv(path + filename)\n",
    "    \n",
    "    for csv in range(start, count): \n",
    "        filename = subreddit + str(csv)\n",
    "        if (subreddit == 'FortNiteBR') & (csv <= 214): \n",
    "            filename = '\\data\\\\' + filename \n",
    "        \n",
    "        df = pd.read_csv(path + filename)\n",
    "        master = pd.concat([master, df])\n",
    "\n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_csvs(subreddit, count, start=0): \n",
    "    path = 'data/' \n",
    "    \n",
    "    for csv in range(start, count): \n",
    "        filename = subreddit + str(csv)\n",
    "        \n",
    "        if (subreddit == 'FortNiteBR') & (csv <= 214): \n",
    "            filename = '\\data\\\\' + filename \n",
    "        \n",
    "        os.remove(path + filename)\n",
    "        \n",
    "    return 'Removed ' + str(count) + ' files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fortniteCompetitive_df = pull_csvs('FortniteCompetitive', 63)\n",
    "fortniteBR_df = pull_csvs('FortNiteBR', 430)\n",
    "\n",
    "fortniteCompetitive_df.to_csv('data/fortniteCompetitive.csv')\n",
    "fortniteBR_df.to_csv('data/fortniteBR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 63 files'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_csvs('FortniteCompetitive', 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 430 files'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_csvs('FortNiteBR', 430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortniteCompetitive_df = pd.read_csv('data/fortniteCompetitive.csv')\n",
    "fortniteBR_df = pd.read_csv('data/fortniteBR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_comments = 'https://oauth.reddit.com/r/FortNiteBR/comments/article'\n",
    "BATCH_SIZE = 20\n",
    "BATCH_NO = 0\n",
    "COLUMNS = ['author', 'body', 'created_utc', 'id', 'parent_id', 'score', 'subreddit', 'permalink']\n",
    "\n",
    "base_headers = {\"Authorization\": \"bearer 72466118-BGb8Kd7V0GF973kFmdLR0aepCdY\", \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
    "headers = set_auth(headers)\n",
    "params = {\n",
    "    'article':'',\n",
    "    'context':8,\n",
    "    'showedits':'True',\n",
    "    'showmore':'False',\n",
    "    'limit' : 1000,\n",
    "    'sort':'confidence',\n",
    "    'threaded':'True',\n",
    "    'truncate':50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint(): \n",
    "    #resume scraping from prev. location or start new \n",
    "    if (os.path.exists('comment_checkpoint.txt')): \n",
    "        with open('comment_checkpoint.txt') as file: \n",
    "            checkpoint = json.load(file)\n",
    "    else: \n",
    "        checkpoint = {'batch_no' : 0}\n",
    "        with open('comment_checkpoint.txt', 'w') as outfile: \n",
    "            json.dump(checkpoint, outfile)\n",
    "            \n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auth():\n",
    "    client_auth = requests.auth.HTTPBasicAuth('NELdoctFZ_tqVw', 'JgSF4SwJdrCsnuOSggxFoMIbzsA')\n",
    "    post_data = {\"grant_type\": \"password\", \"username\": \"jeromeco\", \"password\": \"Skicat12\"}\n",
    "    headers = {\"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
    "    response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "    r = response.json()\n",
    "    return r['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_auth(headers):\n",
    "    headers['Authorization'] = 'bearer ' + get_auth()\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is purposefully blocking\n",
    "def check_limit(id): \n",
    "    params['article'] = id\n",
    "    response = requests.get(url_comments, headers=headers, params=params)\n",
    "    limit = response.headers['x-ratelimit-remaining']\n",
    "    time_remaining = response.headers['x-ratelimit-reset']\n",
    "    return float(limit), float(time_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_limit():\n",
    "    limit, time_remaining = check_limit('8yrb5e')\n",
    "    if limit < BATCH_SIZE: \n",
    "        print('API LIMIT REACHED. \\nSleeping for: ' + str(time_remaining))\n",
    "        time.sleep(time_remaining)\n",
    "    return 'Limit Reset '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2df(response): \n",
    "    comment_queue = response[:] \n",
    "    post = comment_queue.pop(0) # Seed with top-level\n",
    "    comments = []\n",
    "\n",
    "    while comment_queue:\n",
    "        #get comment of queue\n",
    "        try : \n",
    "            comment = comment_queue.pop(0)\n",
    "            comment = comment['data']\n",
    "        except: \n",
    "            comment = comment_queue.pop(0)\n",
    "\n",
    "        #append new comment as a dict to list \n",
    "        if 'body' in comment:\n",
    "            new_comment = {k: comment[k] for k in COLUMNS}    \n",
    "            comments.append(new_comment)\n",
    "\n",
    "        #get children / replies of current comment\n",
    "        if 'children' in comment: \n",
    "            comment = comment['children']\n",
    "            comment_queue.extend(comment)\n",
    "        elif 'replies' in comment: \n",
    "            if len(comment['replies']) > 0: \n",
    "                comment = comment['replies']['data']['children']\n",
    "                comment_queue.extend(comment)\n",
    "        else: \n",
    "            print('error')\n",
    "    \n",
    "    return pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_comment(id, session): \n",
    "    params['article'] = id\n",
    "    async with session.get(url_comments, headers=headers, params=params) as resp:\n",
    "        resp.raise_for_status()\n",
    "        response = await resp.json()  \n",
    "        return json2df(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_batch(batch):  \n",
    "    async with ClientSession() as session:\n",
    "        dfs = []\n",
    "        \n",
    "        for id in batch: \n",
    "            dfs.append(scrape_comment(id, session))\n",
    "        \n",
    "        master = await asyncio.gather(*dfs)\n",
    "        df = pd.concat(master)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_all_comments(posts): \n",
    "    ids = posts['id']\n",
    "    subreddit = posts['subreddit'][0]\n",
    "    checkpoint = get_checkpoint()\n",
    "    \n",
    "    #setup filename structure for saved csvs\n",
    "    path = 'data/' + subreddit +'Comments'\n",
    "    \n",
    "    #iterate through posts to get all comments\n",
    "    x, y = checkpoint['batch_no'], math.ceil(len(ids) / BATCH_SIZE)\n",
    "    for i in tqdm(range(x, y)): \n",
    "        lo = i * BATCH_SIZE\n",
    "        hi = min(len(ids), BATCH_SIZE * (i + 1)) \n",
    "        batch = ids[lo:hi]\n",
    "        \n",
    "        df = await handle_batch(batch)\n",
    "        df.to_csv(path + str(i))\n",
    "        \n",
    "        checkpoint['batch_no'] = i + 1\n",
    "        with open('comment_checkpoint.txt', 'w') as outfile: \n",
    "            json.dump(checkpoint, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8d504a7c4041fcbfcddb94133016ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await get_all_comments(fortniteCompetitive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3c6e1383ba4d9babc28d4dad5e6533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1575), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await get_all_comments(fortniteBR_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 1575 files'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fortniteCompetitiveComments_df = pull_csvs('FortniteCompetitiveComments', 1575)\n",
    "fortniteCompetitiveComments_df.to_csv('data/fortniteCompComments')\n",
    "remove_csvs('FortniteCompetitiveComments', 1575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 10748 files'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fortniteBRComments_df = pull_csvs('FortNiteBRComments', 10748)\n",
    "fortniteCompetitiveComments_df.to_csv('data/FortNiteBRComments')\n",
    "remove_csvs('FortNiteBRComments', 10748)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
