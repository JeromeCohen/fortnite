{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import requests \n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortniteCompetitive_df = pd.read_csv('data/fortniteCompetitive.csv')\n",
    "fortniteBR_df = pd.read_csv('data/fortniteBR.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT = 'FortniteCompetitive'\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "columns = ['author', 'created_utc', 'id', 'num_comments', 'permalink', 'score', 'title', 'selftext', 'subreddit']\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if starting from the beginning\n",
    "# checkpoint = {'date' : 1561584036, \n",
    "#               'count': count}\n",
    "\n",
    "# with open('checkpoint.txt', 'w') as outfile:\n",
    "#     json.dump(checkpoint, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoint.txt') as file: \n",
    "    checkpoint = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'size':'500', \n",
    "        'subreddit': SUBREDDIT, \n",
    "        'num_comments':'>10', \n",
    "        'before' : checkpoint['date']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(params):\n",
    "    response = [1]\n",
    "    count = checkpoint['count']\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, params=params)\n",
    "        print('Status code: ' + str(response.status_code))\n",
    "        response = response.json()\n",
    "        length = len(response['data'])\n",
    "        print('Data length: ' + str(length))\n",
    "        \n",
    "        if length == 0: \n",
    "            print('Scraping finished.')\n",
    "            break\n",
    "        \n",
    "        df = pd.DataFrame(response['data'])\n",
    "        df = df[columns]\n",
    "        \n",
    "        filename = SUBREDDIT + str(count)\n",
    "        path = 'data/' + filename \n",
    "        df.to_csv(path)\n",
    "        print('File named: ' + filename + ' saved')\n",
    "        \n",
    "        count = count + 1\n",
    "        checkpoint['count'] = count\n",
    "        \n",
    "        earliest = length  - 1\n",
    "        checkpoint['date'] = response['data'][earliest]['created_utc']\n",
    "        params['before'] = checkpoint['date']\n",
    "        \n",
    "        with open('checkpoint.txt', 'w') as outfile:\n",
    "            json.dump(checkpoint, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Data length: 0\n"
     ]
    }
   ],
   "source": [
    "scrape(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all separate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_csvs(subreddit, count): \n",
    "    path = 'data/'    \n",
    "    filename = subreddit + str(0)\n",
    "    \n",
    "    if subreddit == 'FortNiteBR': \n",
    "        filename = '\\data\\\\' + filename \n",
    "        \n",
    "    master = pd.read_csv(path + filename)\n",
    "    \n",
    "    for csv in range(1, count): \n",
    "        filename = subreddit + str(csv)\n",
    "        if (subreddit == 'FortNiteBR') & (csv <= 214): \n",
    "            filename = '\\data\\\\' + filename \n",
    "        \n",
    "        df = pd.read_csv(path + filename)\n",
    "        master = pd.concat([master, df])\n",
    "\n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_csvs(subreddit, count): \n",
    "    path = 'data/' \n",
    "    \n",
    "    for csv in range(0, count): \n",
    "        filename = subreddit + str(csv)\n",
    "        \n",
    "        if (subreddit == 'FortNiteBR') & (csv <= 214): \n",
    "            filename = '\\data\\\\' + filename \n",
    "        \n",
    "        os.remove(path + filename)\n",
    "        \n",
    "    return 'Removed ' + str(count) + ' files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortniteCompetitive_df = pull_csvs('FortniteCompetitive', 63)\n",
    "fortniteBR_df = pull_csvs('FortNiteBR', 430)\n",
    "\n",
    "fortniteCompetitive_df.to_csv('data/fortniteCompetitive.csv')\n",
    "fortniteBR_df.to_csv('data/fortniteBR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 63 files'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_csvs('FortniteCompetitive', 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removed 430 files'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_csvs('FortNiteBR', 430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ids = 'https://api.pushshift.io/reddit/submission/comment_ids/'\n",
    "url_comments = 'https://api.pushshift.io/reddit/comment/search'\n",
    "BATCH_SIZE = 50\n",
    "BATCH_NO = 0\n",
    "COLUMNS = ['author', 'body', 'created_utc', 'id', 'parent_id', 'score', 'subreddit', 'total_awards_received', 'permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments(id):\n",
    "    ids = requests.get(url_ids + id).json()['data']\n",
    "    params = {'ids' : ids}\n",
    "    comments = requests.get(url_comments, params=params).json()['data']\n",
    "    comments_df = pd.DataFrame(comments)\n",
    "    comments_df = comments_df[COLUMNS]\n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_comments(ids, index): \n",
    "    lo = (index - 1) * BATCH_SIZE\n",
    "    hi = BATCH_SIZE * index\n",
    "    batch = ids[lo:hi]\n",
    "    \n",
    "    master = pd.DataFrame(columns=COLUMNS)\n",
    "    \n",
    "    for id in batch: \n",
    "        df = scrape_comments(id)\n",
    "        master = pd.concat([master, df])\n",
    "        \n",
    "    return master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
