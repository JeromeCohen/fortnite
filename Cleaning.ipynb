{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_br_df = pd.read_csv('data/fortniteBR.csv')\n",
    "posts_comp_df = pd.read_csv('data/fortniteCompetitive.csv')\n",
    "comments_br_df = pd.read_csv('data/FortNiteBR_Comments')\n",
    "comments_comp_df = pd.read_csv('data/fortniteCompComments')\n",
    "patches_df = pd.read_csv('data/patches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deletion of Rows and Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop extra indexes that were a result of scraping script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_br_df = posts_br_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "posts_comp_df = posts_comp_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "comments_br_df = comments_br_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1',  'Unnamed: 0.1.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_comp_df = comments_comp_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the posts dataframe together and do the same with comments to make cleaning easier. We can always separate them again due to the 'subreddit' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.concat([posts_br_df, posts_comp_df])\n",
    "comments_df = pd.concat([comments_br_df, comments_comp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APIs used return deleted comments and posts. When a author or text is deleted, the text gets replaced with '[deleted]'. We remove these instances from the dataframe. Based on prior inspection not shown, if the selftext / body is deleted the author is not necessarily deleted. However, if the author is deleted the selftext / body always is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df[posts_df['author'] != '[deleted]']\n",
    "comments_df = comments_df[comments_df['author'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now clean the text of posts and commments. This includes removing newline characters and space characters. We are doing this separate from the cleaning / tokenizing since we will use the normal-cased and un-tokenized sentices for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body'] = comments_df['body'].str.replace('\\\\n',' ')\n",
    "comments_df['body'] = comments_df['body'].str.replace('&amp',' ')\n",
    "comments_df['body'] = comments_df['body'].str.replace('&#x200b',' ')\n",
    "posts_df['selftext'] = posts_df['selftext'].str.replace('\\\\n',' ')\n",
    "posts_df['selftext'] = posts_df['selftext'].str.replace('&amp',' ')\n",
    "posts_df['selftext'] = posts_df['selftext'].str.replace('&#x200b',' ')\n",
    "posts_df['title'] = posts_df['title'].str.replace('\\\\n',' ')\n",
    "posts_df['title'] = posts_df['title'].str.replace('&amp',' ')\n",
    "posts_df['title'] = posts_df['title'].str.replace('&#x200b',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization and Lemmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    #make string lowercase \n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    clean_text = []\n",
    "    \n",
    "    #remove stopwords, puncuation, then lemmatize\n",
    "    for word in tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation): \n",
    "            token = wordnet_lemmatizer.lemmatize(word)\n",
    "            clean_text.append(token)\n",
    "            \n",
    "    #remove words of length 3 or smaller        \n",
    "    clean_text = [token for token in clean_text if len(token) > 3] \n",
    "            \n",
    "    return clean_text      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body_clean'] = comments_df['body'].apply(lambda x : clean_text(x))\n",
    "posts_df['selftext_clean'] = posts_df['selftext'].apply(lambda x : clean_text(x))\n",
    "posts_df['title_clean'] = posts_df['title'].apply(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Bi and Tri - grams\n",
    "Inspired by https://www.kaggle.com/ykhorramz/lda-and-t-sne-interactive-visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = posts_df['title_clean'].append(posts_df['selftext_clean'].append(comments_df['body_clean']))\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "trigram = Phrases(bigram[docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc is of type - list. Expecting the tokenized sentences \n",
    "def add_ngram(doc): \n",
    "    for token in bigram[doc]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            doc.append(token)\n",
    "    for token in trigram[doc]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            doc.append(token)\n",
    "    return doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body_clean'] = comments_df['body_clean'].apply(lambda x : add_ngram(x))\n",
    "posts_df['selftext_clean'] = posts_df['selftext_clean'].apply(lambda x : add_ngram(x))\n",
    "posts_df['title_clean'] = posts_df['title_clean'].apply(lambda x : add_ngram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [fortnite, unpopular, opinion, unpopular_opini...\n",
       "1                                       [glad, pump, back]\n",
       "2        [please, epic, remove, combat, shotgun, combat...\n",
       "3        [entitled, section, player, base, play, team, ...\n",
       "4                         [spamming, console, take, skill]\n",
       "5        [made, nothing, much, kinda, proud, made, anot...\n",
       "6                                    [zapatron, unvaulted]\n",
       "7                                         [search, helmet]\n",
       "8        [please, epic, ahead, need, variety, team, mod...\n",
       "9              [clickbait, live, stream, already, rolling]\n",
       "10       [dark, vertex, dark_vertex, dark_vertex, dark_...\n",
       "11       [else, loving, care, think, yesterday, today, ...\n",
       "12                                                  [part]\n",
       "13       [pump, shotgun, available, team, rumble, seem,...\n",
       "14                    [accesories, custom, function, like]\n",
       "15       [release, duel, pickax, nunchucks, shop, spend...\n",
       "16       [weapon, concept, bouncy, sniper, rifle, snipe...\n",
       "17                                    [drift, grew, beard]\n",
       "18       [coming, soon, favorite, bundle, time, coming_...\n",
       "19                                           [game, sound]\n",
       "20                                           [every, time]\n",
       "21         [anyone, know, leviathan, come, back, fortnite]\n",
       "22                      [toxic, player, challenge, camper]\n",
       "23       [shadowbird, shadow, skully, offer, normal, co...\n",
       "24                                  [rotation, gone, gone]\n",
       "25       [there, actual, face, picture, frame, salty, r...\n",
       "26                                     [team, rumbe, gone]\n",
       "27                                   [look, friend, found]\n",
       "28                                     [hoverboard, sling]\n",
       "29       [look, change, even, though, love, skin, hope,...\n",
       "                               ...                        \n",
       "31467                          [banned, item, competitive]\n",
       "31468    [solution, nerf, double, pump, double_pump, do...\n",
       "31469                          [could, make, better, team]\n",
       "31470    [competitive, scene, console, competitive_scen...\n",
       "31471                  [team, support, tournament, arrive]\n",
       "31472    [advanced, similar, ghost, peeking, ghost_peek...\n",
       "31473    [introducing, fortnite, custom, level, calcula...\n",
       "31474    [hosting, test, tournament, x1/pc, weekend, wa...\n",
       "31475                                             [season]\n",
       "31476    [supply, drop, could, shake, meta, supply_drop...\n",
       "31477    [anyone, else, think, showdown, mode, epic, in...\n",
       "31478                                      [player, think]\n",
       "31479    [ideal, setup, competitive, fortnite, instead,...\n",
       "31480                         [tfue, show, strat, pyramid]\n",
       "31481       [shall, list, team, associated, player, going]\n",
       "31482           [idea, reward, aggressive, play, fortnite]\n",
       "31483    [escaping, situation, ground, build, battle, b...\n",
       "31484    [epic, look, introduce, another, incentive, re...\n",
       "31485    [team, think, dominate, competitive, scene, co...\n",
       "31486    [really, behind, actual, competitive, start, s...\n",
       "31487    [fortnite, want, start, tournament, competitiv...\n",
       "31488                                 [building, keybinds]\n",
       "31489    [want, maintain, even, improve, skill, fortnit...\n",
       "31490    [today, change, shotgun, think, optimal, lineu...\n",
       "31491                [thought, current, competitive, meta]\n",
       "31492    [scrimming, anymore, fortnite, competitive, fo...\n",
       "31493                                         [best, noob]\n",
       "31494                            [team, liquid, announced]\n",
       "31495             [shotgun/trap, balance, jetpack, update]\n",
       "31496    [welcome, /r/fortnitecompetitive, welcome_/r/f...\n",
       "Name: title_clean, Length: 244495, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df['title_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
