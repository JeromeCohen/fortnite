{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jeromecohen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_br_df = pd.read_csv('data/fortniteBR.csv')\n",
    "posts_comp_df = pd.read_csv('data/fortniteCompetitive.csv')\n",
    "comments_br_df = pd.read_csv('data/FortNiteBR_Comments')\n",
    "comments_comp_df = pd.read_csv('data/fortniteCompComments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deletion of Rows and Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop extra indexes that were a result of scraping script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_br_df = posts_br_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "posts_comp_df = posts_comp_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "comments_br_df = comments_br_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1',  'Unnamed: 0.1.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_comp_df = comments_comp_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the posts dataframe together and do the same with comments to make cleaning easier. We can always separate them again due to the 'subreddit' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.concat([posts_br_df, posts_comp_df])\n",
    "comments_df = pd.concat([comments_br_df, comments_comp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APIs used return deleted comments and posts. When a author or text is deleted, the text gets replaced with '[deleted]'. We remove these instances from the dataframe. Based on prior inspection not shown, if the selftext / body is deleted the author is not necessarily deleted. However, if the author is deleted the selftext / body always is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df[posts_df['author'] != '[deleted]']\n",
    "comments_df = comments_df[comments_df['author'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now clean the text of posts and commments. This includes removing newline characters and space characters. We are doing this separate from the cleaning / tokenizing since we will use the normal-cased and un-tokenized sentices for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_clean(text): \n",
    "    text = str(text)\n",
    "    text = text.replace('\\\\n',' ')\n",
    "    text = text.replace('&amp',' ')\n",
    "    text = text.replace(';#x200B;',' ')\n",
    "    text = text.replace('nbsp',' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body'] = comments_df['body'].apply(lambda x : first_clean(x))\n",
    "posts_df['selftext'] = posts_df['selftext'].apply(lambda x : first_clean(x))\n",
    "posts_df['title'] = posts_df['title'].apply(lambda x : first_clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization and Lemmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    #make string lowercase \n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove links\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    clean_text = []\n",
    "    \n",
    "    #remove stopwords, puncuation, then lemmatize\n",
    "    for word in tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation): \n",
    "            token = wordnet_lemmatizer.lemmatize(word)\n",
    "            clean_text.append(token)\n",
    "            \n",
    "    #remove words of length 3 or smaller        \n",
    "    clean_text = [token for token in clean_text if len(token) > 3] \n",
    "            \n",
    "    return clean_text      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body_clean'] = comments_df['body'].apply(lambda x : clean_text(x))\n",
    "posts_df['selftext_clean'] = posts_df['selftext'].apply(lambda x : clean_text(x))\n",
    "posts_df['title_clean'] = posts_df['title'].apply(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove posts / comments that are less then length 5 after cleaning. The dataset currently contains many one-word comments like 'nice' that only provide noise to the models we will be using. By removing short selftext, we are also removing posts that only have titles (since they are normally images and now NaN in our dataset). Titles only have to be length 3 or longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body_length'] = comments_df['body_clean'].apply(lambda x : len(x))\n",
    "posts_df['selftext_length'] = posts_df['selftext_clean'].apply(lambda x : len(x))\n",
    "posts_df['title_length'] = posts_df['title_clean'].apply(lambda x : len(x))\n",
    "\n",
    "comments_df = comments_df[comments_df['body_length'] >= 5]\n",
    "posts_df = posts_df[((posts_df['selftext_length'] >= 5) & (posts_df['title_length'] >= 3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df.drop(columns=['body_length'])\n",
    "posts_df = posts_df.drop(columns=['title_length','selftext_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Bi and Tri - grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.concat([posts_df['title_clean'], posts_df['selftext_clean'], comments_df['body_clean']])\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "trigram = Phrases(bigram[docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc is of type - list. Expecting the tokenized sentences \n",
    "def add_ngram(doc): \n",
    "    return trigram[bigram[doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body_ngrams'] = comments_df['body_clean'].apply(lambda x : add_ngram(x))\n",
    "posts_df['selftext_ngrams'] = posts_df['selftext_clean'].apply(lambda x : add_ngram(x))\n",
    "posts_df['title_ngrams'] = posts_df['title_clean'].apply(lambda x : add_ngram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_pickle('data/comments')\n",
    "posts_df.to_pickle('data/posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
